# ğŸ™ï¸ Voice AI Backend

> **A production-grade backend for a Voice -> STT -> LLM -> TTS pipeline with conversational AI, memory, and skills.**

![Status](https://img.shields.io/badge/Status-Active-success)
![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.95%2B-009688)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15%2B-336791)

---

## ğŸ“– Overview

This project implements a robust backend for a Voice AI assistant. It handles real-time audio streaming, speech-to-text transcription, intelligent LLM processing with context-aware memory, and text-to-speech synthesis. It is designed to be modular, scalable, and easily extensible with new skills and LLM providers.

## ğŸ—ï¸ Architecture

The system is built on a microservices-like architecture within a modular monolith.

```mermaid
graph TD
    subgraph "Frontend"
        FlutterApp[ğŸ“± Flutter Mobile App]
    end

    subgraph "Backend (FastAPI)"
        API[ğŸ”Œ API Gateway / WebSocket]
        Auth[ğŸ›¡ï¸ Auth Service]
        Orchestrator[ğŸ§  LLM Orchestrator]
        Skills[ğŸ› ï¸ Skill Registry]
        MemoryMgr[ğŸ’¾ Memory Manager]
    end

    subgraph "Data Layer"
        Postgres[(ğŸ˜ PostgreSQL\nUser Profiles & Logs)]
        Redis[(âš¡ Redis\nShort-term Memory & Cache)]
        VectorDB[(ğŸ” Vector DB\nLong-term Memory)]
    end

    subgraph "External Services"
        LLM_API[ğŸ¤– LLM Providers\n(OpenAI, Gemini, Local)]
        STT_API[ğŸ—£ï¸ STT Service]
        TTS_API[ğŸ”ˆ TTS Service]
    end

    FlutterApp <-->|WebSocket / HTTP| API
    API --> Auth
    API --> Orchestrator
    Orchestrator --> MemoryMgr
    Orchestrator --> Skills
    Orchestrator <--> LLM_API
    Orchestrator <--> STT_API
    Orchestrator <--> TTS_API
    MemoryMgr <--> Postgres
    MemoryMgr <--> Redis
    MemoryMgr <--> VectorDB
```

### Key Components
- **FastAPI**: High-performance Async I/O for REST and WebSockets.
- **WebSockets**: Real-time bi-directional audio streaming.
- **Modular LLM Engine**: Supports OpenAI, Gemini, and Local LLaMA.
- **Memory System**:
    - **Short-term**: Redis (Conversation context).
    - **Long-term**: Vector DB (Semantic search).
    - **Profile**: Postgres (User preferences).
- **Skill System**: Extensible registry for actions (Calling, SMS, Navigation).

---

## ğŸ”„ Process Flow

The following diagram illustrates the lifecycle of a voice interaction:

```mermaid
sequenceDiagram
    participant User
    participant App as ğŸ“± Flutter App
    participant WS as ğŸ”Œ WebSocket Endpoint
    participant Orch as ğŸ§  Orchestrator
    participant LLM as ğŸ¤– LLM Provider
    participant Mem as ğŸ’¾ Memory
    participant TTS as ğŸ”ˆ TTS Engine

    User->>App: Speaks ğŸ—£ï¸
    App->>WS: Streams Audio Chunks ğŸŒŠ
    WS->>Orch: Transcribes Audio (STT)
    Orch->>Mem: Retrieve Context ğŸ”
    Mem-->>Orch: Context Data
    Orch->>LLM: Send Prompt + Context
    LLM-->>Orch: Generated Response ğŸ“
    
    alt Skill Triggered
        Orch->>Orch: Execute Skill ğŸ› ï¸
    end

    Orch->>TTS: Synthesize Response
    TTS-->>WS: Audio Stream ğŸµ
    WS-->>App: Play Audio
    App->>User: Hears Response ğŸ‘‚
```

---

## ğŸ“± Wireframes & UI Flow

The mobile application follows a clean, intuitive flow designed for voice-first interaction.

```mermaid
graph LR
    Splash[ğŸš€ Splash Screen] --> Auth[ğŸ” Login / Register]
    Auth --> Home[ğŸ  Home Dashboard]
    
    subgraph "Main Interface"
        Home --> VoiceMode[ğŸ™ï¸ Voice Chat Mode\n(Main Interaction)]
        Home --> History[ğŸ“œ Chat History]
        Home --> Settings[âš™ï¸ Settings\n(Voice, Persona, API Keys)]
    end

    VoiceMode -->|Active Call| ActiveState[ğŸ”´ Listening / Speaking]
    ActiveState -->|End| Home
```

---

## ğŸš€ Getting Started

### Prerequisites
- **Python 3.10+**
- **PostgreSQL**
- **Redis**
- **Milvus** (or other Vector DB)

### Installation

1. **Clone the repository**
   ```bash
   git clone <repo-url>
   cd backend
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Environment Setup**
   Copy `.env.example` to `.env` and update the values.
   ```bash
   cp .env.example .env
   ```

### Database Setup

1. **Create the PostgreSQL database**
   ```sql
   CREATE DATABASE voice_ai;
   ```

2. **Run Migrations**
   Ensure your database is running and configured in `.env`.

### Running the Application

Start the server with hot-reload:
```bash
uvicorn backend.main:app --reload
```

- **API**: `http://localhost:8000`
- **Docs**: `http://localhost:8000/docs`

---

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
| :--- | :--- | :--- |
| **POST** | `/api/v1/auth/register` | Register a new user |
| **POST** | `/api/v1/auth/token` | Login and get access token |
| **POST** | `/api/v1/chat/message` | Send a text message |
| **WS** | `/api/v1/ws/stream` | Real-time voice stream |
| **GET** | `/api/v1/skills/` | List available skills |
| **POST** | `/api/v1/memory/search` | Semantic search in memory |

---

## ğŸ“‚ Project Structure

```
backend/
â”œâ”€â”€ api/            # REST routes and WebSocket handlers
â”œâ”€â”€ llm/            # Orchestrator, Prompt Builder, Providers
â”œâ”€â”€ memory/         # Redis, Vector DB, Postgres controllers
â”œâ”€â”€ skills/         # Action registry and implementations
â”œâ”€â”€ db/             # Database models and session
â””â”€â”€ core/           # Config and security
```

---

Made with â¤ï¸ by the Voice AI Team
